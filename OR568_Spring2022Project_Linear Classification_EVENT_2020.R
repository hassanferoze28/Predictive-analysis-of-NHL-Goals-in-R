#' ---
#' Team: Chang,Donde,Hassan,Karimi,Miller,Turissini    
#' title: "OR 568 - Spring 2022, Prof Xu"
#' Project
#' Due: May 10, 2022 11:59pm
#'


################################################################################
##############################  Events 2020  ###################################
################################################################################
################################################################################
### Linear Classification
# 1 Logistic Regression
# 2 Linear Discriminant Analysis
# 3 Partial Least Squares Discriminant Analysis
# 4 glmnet
# 5 Sparse logistic regression
# 6 Nearest Shrunken Centroids

library(caret)
library(doMC)
library(pROC)
library(glmnet)
library(lattice)
library(MASS)
library(pamr) 
library(pls)
library(sparseLDA)
library(MLmetrics)


# Import the dataset
NHL_df_event = read.csv("C:/Users/m29336/Documents/NHL_df_event.csv", header=TRUE)
dim(NHL_df_event)        # 78582   88

NHL_df_event$event = as.factor(NHL_df_event$event) #change to factor
class(NHL_df_event$event)

#set Control
ctrl = trainControl(method = "repeatedcv", repeats = 5, summaryFunction=multiClassSummary, classProbs=TRUE )

## Split the data into training (80%) and test sets (20%)
set.seed(123)
inTrain_event <- createDataPartition(NHL_df_event$event, p = .8)[[1]]
Train.NHL_df_event <- NHL_df_event[ inTrain_event, ]
Test.NHL_df_event  <- NHL_df_event[-inTrain_event, ]


################################################################################
### 1 Logistic Regression

set.seed(123)
logisticReg_event <- train(event ~ .,
                     data = Train.NHL_df_event,
                     method = "multinom",
                     trControl = ctrl)
logisticReg_event

# Penalized Multinomial Regression 
# 
# 62867 samples
# 87 predictor
# 3 classes: 'GOAL', 'MISS', 'SHOT' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 56580, 56580, 56580, 56580, 56582, 56580, ... 
# Resampling results across tuning parameters:
#   
#   decay  logLoss    AUC        prAUC      Accuracy   Kappa      Mean_F1    Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value
# 0e+00  0.5532888  0.8187621  0.7146785  0.7179123  0.3181544  0.6103170  0.6746574         0.7505126         0.6663315            0.8293842          
# 1e-04  0.5532573  0.8188654  0.7147259  0.7178487  0.3180714  0.6103512  0.6746933         0.7504985         0.6658129            0.8292386          
# 1e-01  0.5530849  0.8191028  0.7146069  0.7181192  0.3189949  0.6107951  0.6750728         0.7508061         0.6665602            0.8294622          
# Mean_Precision  Mean_Recall  Mean_Detection_Rate  Mean_Balanced_Accuracy
# 0.6663315       0.6746574    0.2393041            0.7125850             
# 0.6658129       0.6746933    0.2392829            0.7125959             
# 0.6665602       0.6750728    0.2393731            0.7129395             
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was decay = 0.1.

logisticRegImp <- varImp(logisticReg_event, scale = FALSE)
logisticRegImp

# multinom variable importance
# 
# only 20 most important variables shown (out of 87)
# 
# Overall
# defendingTeamAverageTimeOnIceOfDefencemen            26.638
# defendingTeamMaxTimeOnIceOfDefencemen                16.549
# defendingTeamMinTimeOnIceOfDefencemen                16.436
# defendingTeamAverageTimeOnIce                        10.068
# defendingTeamAverageTimeOnIceOfForwards               5.362
# defendingTeamAverageTimeOnIceOfForwardsSinceFaceoff   5.170
# shotPlayContinuedOutsideZone                          4.800
# shotPlayContinuedInZone                               4.606
# defendingTeamAverageTimeOnIceSinceFaceoff             4.151
# defendingTeamMinTimeOnIceOfForwards                   3.679
# defendingTeamMaxTimeOnIceOfForwards                   3.574
# defendingTeamMinTimeOnIceOfDefencemenSinceFaceoff     3.200
# shotGoalieFroze                                       3.083
# shotGeneratedRebound                                  1.854
# defendingTeamMaxTimeOnIce                             1.682
# defendingTeamMaxTimeOnIceOfForwardsSinceFaceoff       1.417
# defendingTeamMinTimeOnIceOfForwardsSinceFaceoff       1.405
# shootingTeamAverageTimeOnIceSinceFaceoff              1.365
# shootingTeamDefencemenOnIce                           1.313
# shootingTeamForwardsOnIce                             1.253

plot(logisticRegImp, top=15, scales = list(y = list(cex = .85)))

### Predict the test set
goal.Results_LR <- data.frame(obs = Test.NHL_df_event$event)
goal.Results_LR$prob <- predict(logisticReg_event, Test.NHL_df_event, type = "prob")[, "GOAL"]
goal.Results_LR$pred <- predict(logisticReg_event, Test.NHL_df_event)
goal.Results_LR$Label <- ifelse(goal.Results_LR$obs == "GOAL",
                                "True Outcome: GOAL",
                                "True Outcome: MISS",
                                "True Outcome: SHOT")


goal.Results_LR$obs <- as.factor(goal.Results_LR$obs)

defaultSummary(goal.Results_LR)
# Accuracy     Kappa 
# 0.6978683 0.2793856 

### Create the confusion matrix from the test set.
confusionMatrix(data = goal.Results_LR$pred,
                reference = goal.Results_LR$obs,
                positive = "Goal")

# Confusion Matrix and Statistics
# 
# Reference
# Prediction GOAL MISS SHOT
# GOAL 1016  165  231
# MISS   23  418  599
# SHOT   56 3674 9533
# 
# Overall Statistics
# 
# Accuracy : 0.6979         
# 95% CI : (0.6906, 0.705)
# No Information Rate : 0.6594         
# P-Value [Acc > NIR] : < 2.2e-16      
# 
# Kappa : 0.2794         
# 
# Mcnemar's Test P-Value : < 2.2e-16      
# 
# Statistics by Class:
# 
#                      Class: GOAL Class: MISS Class: SHOT
# Sensitivity              0.92785     0.09819      0.9199
# Specificity              0.97291     0.94571      0.3031
# Pos Pred Value           0.71955     0.40192      0.7188
# Neg Pred Value           0.99448     0.73840      0.6615
# Prevalence               0.06968     0.27089      0.6594
# Detection Rate           0.06465     0.02660      0.6066
# Detection Prevalence     0.08985     0.06618      0.8440
# Balanced Accuracy        0.95038     0.52195      0.6115

### ROC curves:
goal.ROC_LR <- multiclass.roc(response = goal.Results_LR$obs, predictor = goal.Results_LR$prob, levels = levels(goal.Results_LR$obs))

auc(goal.ROC_LR) # Multi-class area under the curve: 0.8304

### Note the x-axis is reversed
plot(goal.ROC_LR)

### Lift charts
goal.Lift_LR <- lift(obs ~ prob, data = goal.Results_LR)
xyplot(goal.Lift_LR)

################################################################################
### 2 Linear Discriminant Analysis

set.seed(123)
ldaFit <- train(event ~ .,
                data = Train.NHL_df_event,
                method = "lda",
                metric = "ROC",
                trControl = ctrl)
ldaFit

# Linear Discriminant Analysis 
# 
# 62867 samples
# 87 predictor
# 3 classes: 'GOAL', 'MISS', 'SHOT' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold, repeated 5 times) 
# Summary of sample sizes: 56580, 56580, 56580, 56580, 56582, 56580, ... 
# Resampling results:
#   
#   logLoss    AUC       prAUC      Accuracy   Kappa      Mean_F1    Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value  Mean_Precision  Mean_Recall  Mean_Detection_Rate
# 0.9154834  0.820344  0.7242504  0.7213356  0.3326753  0.6205159  0.6847789         0.75536           0.6743978            0.8297445            0.6743978       0.6847789    0.2404452          
# Mean_Balanced_Accuracy
# 0.7200694 

ldaImp <- varImp(ldaFit, scale = FALSE)
ldaImp

# ROC curve variable importance
# 
# variables are sorted by maximum importance across the classes
# only 20 most important variables shown (out of 87)
# 
# GOAL   MISS   SHOT
# shotPlayContinuedOutsideZone           0.7490 0.7490 0.6834
# shotDistance                           0.7220 0.7220 0.6926
# shotPlayContinuedInZone                0.7092 0.7092 0.6489
# arenaAdjustedXCordABS                  0.6864 0.6864 0.6547
# arenaAdjustedYCordAbs                  0.6820 0.6820 0.6717
# shotGoalieFroze                        0.6241 0.6190 0.6241
# timeSinceLastEvent                     0.6046 0.6046 0.5800
# playerPositionThatDidEventD            0.5929 0.5929 0.5865
# speedFromLastEvent                     0.5720 0.5720 0.5450
# shotRebound                            0.5618 0.5618 0.5499
# playerPositionThatDidEventC            0.5575 0.5575 0.5514
# shotAngleAdjusted                      0.5572 0.5451 0.5572
# shotAnglePlusReboundSpeed              0.5481 0.5481 0.5459
# distanceFromLastEvent                  0.5454 0.5405 0.5454
# shootingTeamAverageTimeOnIce           0.5415 0.5235 0.5415
# shootingTeamAverageTimeOnIceOfForwards 0.5379 0.5205 0.5379
# shotAnglePlusRebound                   0.5378 0.5361 0.5378
# time                                   0.5370 0.5325 0.5370
# shootingTeamMaxTimeOnIce               0.5360 0.5263 0.5360
# shootingTeamMaxTimeOnIceOfForwards     0.5358 0.5219 0.5358

plot(ldaImp, top=15, scales = list(y = list(cex = .85)))

### Predict the test set
goal.Results_ldaFit <- data.frame(obs = Test.NHL_df_event$event)
goal.Results_ldaFit$prob <- predict(ldaFit, Test.NHL_df_event, type = "prob")[, "GOAL"]
goal.Results_ldaFit$pred <- predict(ldaFit, Test.NHL_df_event)
goal.Results_ldaFit$Label <- ifelse(goal.Results_ldaFit$obs == "GOAL",
                                    "True Outcome: GOAL",
                                    "True Outcome: MISS",
                                    "True Outcome: SHOT")


goal.Results_ldaFit$obs <- as.factor(goal.Results_ldaFit$obs)

defaultSummary(goal.Results_ldaFit)
# Accuracy     Kappa 
# 0.7205218 0.3313506 

### Create the confusion matrix from the test set.
confusionMatrix(data = goal.Results_ldaFit$pred,
                reference = goal.Results_ldaFit$obs)

# Confusion Matrix and Statistics
# 
# Reference
# Prediction GOAL MISS SHOT
# GOAL 1094  175  233
# MISS    0  475  376
# SHOT    1 3607 9754
# 
# Overall Statistics
# 
# Accuracy : 0.7205          
# 95% CI : (0.7134, 0.7275)
# No Information Rate : 0.6594          
# P-Value [Acc > NIR] : < 2.2e-16       
# 
# Kappa : 0.3314          
# 
# Mcnemar's Test P-Value : < 2.2e-16       
# 
# Statistics by Class:
# 
#                      Class: GOAL Class: MISS Class: SHOT
# Sensitivity              0.99909     0.11158      0.9412
# Specificity              0.97209     0.96718      0.3259
# Pos Pred Value           0.72836     0.55817      0.7300
# Neg Pred Value           0.99993     0.74556      0.7412
# Prevalence               0.06968     0.27089      0.6594
# Detection Rate           0.06962     0.03023      0.6207
# Detection Prevalence     0.09558     0.05415      0.8503
# Balanced Accuracy        0.98559     0.53938      0.6335          

### ROC curves:
goal.ROC_ldaFit <- multiclass.roc(response = goal.Results_ldaFit$obs, predictor = goal.Results_ldaFit$prob, levels = levels(goal.Results_ldaFit$obs))

auc(goal.ROC_ldaFit) #Multi-class area under the curve: 0.8351


################################################################################
### 3 Partial Least Squares Discriminant Analysis

set.seed(123)

plsFit <- train(event ~ .,
                data = Train.NHL_df_event,
                method = "pls",
                tuneGrid = expand.grid(ncomp = 1:50),
                trControl = ctrl)
plsFit

# Partial Least Squares 
# 
# # 62867 samples
# 87 predictor
# 3 classes: 'GOAL', 'MISS', 'SHOT' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 56580, 56580, 56580, 56580, 56582, 56580, ... 
# Resampling results across tuning parameters:
#   
#   ncomp  logLoss    AUC        prAUC      Accuracy   Kappa       Mean_F1    Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value  Mean_Precision  Mean_Recall  Mean_Detection_Rate
# 1     0.9378048  0.5943420  0.3977855  0.6560994  0.01511075        NaN  0.3377657         0.6705643               NaN            0.7101021                  NaN       0.3377657    0.2186998          
# 2     0.9178455  0.6120273  0.4286080  0.6640208  0.05860040        NaN  0.3517236         0.6815113               NaN            0.7509778                  NaN       0.3517236    0.2213403          
# 3     0.9075062  0.7426378  0.5267148  0.6648320  0.06504657  0.3198928  0.3556592         0.6833792         0.5927175            0.7566732            0.5927175       0.3556592    0.2216107          
# 4     0.8802429  0.8052793  0.6895756  0.7035647  0.24309810  0.5576896  0.5776250         0.7274018         0.6724790            0.8252632            0.6724790       0.5776250    0.2345216          
# 5     0.8676679  0.8104635  0.7003992  0.7170217  0.30138320  0.5899178  0.6634243         0.7439977         0.6709023            0.8402257            0.6709023       0.6634243    0.2390072          
# 6     0.8579377  0.8133946  0.7047271  0.7187715  0.30409965  0.5863906  0.6697448         0.7444878         0.6790804            0.8470948            0.6790804       0.6697448    0.2395905          
# 7     0.8555092  0.8157618  0.7069927  0.7188986  0.30554769  0.5883127  0.6704781         0.7450255         0.6778632            0.8458862            0.6778632       0.6704781    0.2396329          
# 8     0.8541643  0.8172448  0.7142510  0.7187396  0.30245109  0.5840102  0.6688751         0.7438577         0.6812617            0.8489192            0.6812617       0.6688751    0.2395799          
# 9     0.8533986  0.8184939  0.7122536  0.7202666  0.31071084  0.5927921  0.6727472         0.7467046         0.6848526            0.8460116            0.6848526       0.6727472    0.2400889          
# 10     0.8532221  0.8188069  0.7169813  0.7200439  0.30929612  0.5911643  0.6722390         0.7461978         0.6845860            0.8466741            0.6845860       0.6722390    0.2400146          
# 11     0.8530437  0.8188990  0.7172131  0.7201871  0.31556844  0.5999466  0.6756333         0.7486527         0.6777160            0.8404692            0.6777160       0.6756333    0.2400624          
# 12     0.8528324  0.8195743  0.7187755  0.7201871  0.31754708  0.6027647  0.6767406         0.7494475         0.6762575            0.8385169            0.6762575       0.6767406    0.2400624          
# 13     0.8524794  0.8205322  0.7198096  0.7198690  0.31414200  0.5985081  0.6749765         0.7481702         0.6771466            0.8407556            0.6771466       0.6749765    0.2399563          
# 14     0.8523250  0.8210883  0.7204216  0.7192804  0.30839171  0.5914846  0.6720953         0.7460427         0.6778249            0.8444236            0.6778249       0.6720953    0.2397601          
# 15     0.8522075  0.8212894  0.7216673  0.7192963  0.30829269  0.5912990  0.6720341         0.7460002         0.6781848            0.8446066            0.6781848       0.6720341    0.2397654          
# 16     0.8520882  0.8215871  0.7223251  0.7196304  0.30829033  0.5905281  0.6718686         0.7459124         0.6816219            0.8460870            0.6816219       0.6718686    0.2398768          
# 17     0.8519776  0.8218497  0.7225835  0.7202189  0.31169860  0.5942095  0.6734579         0.7471106         0.6825727            0.8447480            0.6825727       0.6734579    0.2400730          
# 18     0.8519378  0.8219084  0.7231847  0.7200598  0.31170825  0.5945472  0.6735389         0.7471556         0.6810253            0.8440788            0.6810253       0.6735389    0.2400199          
# 19     0.8518984  0.8219653  0.7240805  0.7198689  0.31142883  0.5945705  0.6734770         0.7470932         0.6795955            0.8435646            0.6795955       0.6734770    0.2399563          
# 20     0.8518609  0.8221651  0.7238010  0.7201234  0.31260058  0.5957009  0.6739978         0.7474916         0.6807910            0.8433945            0.6807910       0.6739978    0.2400411          
# 21     0.8518464  0.8221879  0.7243517  0.7198689  0.31189139  0.5952424  0.6737308         0.7472758         0.6793216            0.8430872            0.6793216       0.6737308    0.2399563          
# 22     0.8518307  0.8221771  0.7245717  0.7202984  0.31264040  0.5953705  0.6739364         0.7474638         0.6822340            0.8440669            0.6822340       0.6739364    0.2400995          
# 23     0.8518223  0.8222199  0.7250319  0.7203143  0.31231479  0.5948580  0.6737483         0.7473300         0.6829124            0.8445032            0.6829124       0.6737483    0.2401048          
# 24     0.8518206  0.8222114  0.7248847  0.7202666  0.31222852  0.5948431  0.6737242         0.7473082         0.6824787            0.8443808            0.6824787       0.6737242    0.2400889          
# 25     0.8518237  0.8222207  0.7248816  0.7202984  0.31262020  0.5953410  0.6739248         0.7474555         0.6822240            0.8440940            0.6822240       0.6739248    0.2400995          
# 26     0.8518119  0.8222384  0.7248830  0.7203143  0.31289770  0.5957088  0.6740712         0.7475623         0.6819668            0.8438509            0.6819668       0.6740712    0.2401048          
# 27     0.8518026  0.8222940  0.7249574  0.7201712  0.31238897  0.5952901  0.6738605         0.7473973         0.6812234            0.8437921            0.6812234       0.6738605    0.2400571          
# 28     0.8517955  0.8223326  0.7249983  0.7202348  0.31242145  0.5951963  0.6738465         0.7473932         0.6818343            0.8440269            0.6818343       0.6738465    0.2400783 ****          
# 29     0.8517979  0.8222970  0.7249468  0.7201871  0.31237952  0.5952412  0.6738455         0.7473880         0.6815071            0.8438825            0.6815071       0.6738455    0.2400624          
# 30     0.8518025  0.8222433  0.7248810  0.7199803  0.31202965  0.5951912  0.6737525         0.7473018         0.6803400            0.8434074            0.6803400       0.6737525    0.2399934          
# 31     0.8518004  0.8222488  0.7249268  0.7200121  0.31160704  0.5945032  0.6735033         0.7471256         0.6810974            0.8440064            0.6810974       0.6735033    0.2400040          
# 32     0.8518082  0.8221878  0.7248573  0.7200121  0.31173135  0.5946853  0.6735725         0.7471753         0.6808672            0.8438666            0.6808672       0.6735725    0.2400040          
# 33     0.8518097  0.8221916  0.7247935  0.7198849  0.31162759  0.5948187  0.6735774         0.7471669         0.6797989            0.8434375            0.6797989       0.6735774    0.2399616          
# 34     0.8518076  0.8221969  0.7247098  0.7197576  0.31116943  0.5944366  0.6733862         0.7470175         0.6792598            0.8433989            0.6792598       0.6733862    0.2399192          
# 35     0.8518055  0.8222102  0.7246950  0.7198053  0.31123365  0.5944235  0.6733988         0.7470311         0.6795214            0.8435295            0.6795214       0.6733988    0.2399351          
# 36     0.8518034  0.8222277  0.7247293  0.7198689  0.31136830  0.5944766  0.6734425         0.7470684         0.6799718            0.8436506            0.6799718       0.6734425    0.2399563          
# 37     0.8517984  0.8222674  0.7247787  0.7199326  0.31156671  0.5946242  0.6735208         0.7471307         0.6803313            0.8437029            0.6803313       0.6735208    0.2399775          
# 38     0.8517993  0.8222670  0.7247691  0.7199167  0.31151836  0.5945877  0.6735012         0.7471151         0.6803017            0.8436954            0.6803017       0.6735012    0.2399722          
# 39     0.8518001  0.8222591  0.7247624  0.7199803  0.31165405  0.5946410  0.6735449         0.7471525         0.6807039            0.8438190            0.6807039       0.6735449    0.2399934          
# 40     0.8518024  0.8222455  0.7247698  0.7199485  0.31159611  0.5946287  0.6735288         0.7471380         0.6805168            0.8437539            0.6805168       0.6735288    0.2399828          
# 41     0.8518017  0.8222400  0.7247442  0.7199644  0.31164615  0.5946659  0.6735484         0.7471535         0.6806160            0.8437636            0.6806160       0.6735484    0.2399881          
# 42     0.8518032  0.8222360  0.7248128  0.7199485  0.31163790  0.5946900  0.6735519         0.7471545         0.6804647            0.8437048            0.6804647       0.6735519    0.2399828          
# 43     0.8518042  0.8222337  0.7248151  0.7198849  0.31148105  0.5946044  0.6734967         0.7471088         0.6801272            0.8436110            0.6801272       0.6734967    0.2399616          
# 44     0.8518042  0.8222243  0.7247881  0.7199167  0.31155873  0.5946487  0.6735243         0.7471317         0.6802629            0.8436572            0.6802629       0.6735243    0.2399722          
# 45     0.8518033  0.8222329  0.7247727  0.7200598  0.31202552  0.5950079  0.6737120         0.7472801         0.6809219            0.8437421            0.6809219       0.6737120    0.2400199          
# 46     0.8518010  0.8222454  0.7247940  0.7197894  0.31128866  0.5945391  0.6734369         0.7470569         0.6794336            0.8434153            0.6794336       0.6734369    0.2399298          
# 47     0.8518011  0.8222425  0.7248063  0.7199167  0.31164332  0.5947719  0.6735704         0.7471649         0.6802025            0.8435624            0.6802025       0.6735704    0.2399722          
# 48     0.8518037  0.8222267  0.7247635  0.7199326  0.31173503  0.5948698  0.6736130         0.7471970         0.6802601            0.8435294            0.6802601       0.6736130    0.2399775          
# 49     0.8518043  0.8222066  0.7246914  0.7199008  0.31165750  0.5948274  0.6735854         0.7471742         0.6800207            0.8434785            0.6800207       0.6735854    0.2399669          
# 50     0.8518015  0.8222097  0.7246983  0.7198371  0.31143802  0.5946501  0.6734956         0.7471037         0.6797323            0.8434476            0.6797323       0.6734956    0.2399457          
# Mean_Balanced_Accuracy
# 0.5041650             
# 0.5166175             
# 0.5195192             
# 0.6525134             
# 0.7037110             
# 0.7071163             
# 0.7077518             
# 0.7063664             
# 0.7097259             
# 0.7092184             
# 0.7121430             
# 0.7130941             
# 0.7115733             
# 0.7090690             
# 0.7090172             
# 0.7088905             
# 0.7102842             
# 0.7103472             
# 0.7102851             
# 0.7107447             
# 0.7105033             
# 0.7107001             
# 0.7105392             
# 0.7105162             
# 0.7106901             
# 0.7108168             
# 0.7106289             
# 0.7106199             
# 0.7106168             
# 0.7105272             
# 0.7103144             
# 0.7103739             
# 0.7103722             
# 0.7102019             
# 0.7102149             
# 0.7102555             
# 0.7103257             
# 0.7103082             
# 0.7103487             
# 0.7103334             
# 0.7103510             
# 0.7103532             
# 0.7103028             
# 0.7103280             
# 0.7104960             
# 0.7102469             
# 0.7103676             
# 0.7104050             
# 0.7103798             
# 0.7102996             
# 
# logLoss was used to select the optimal model using the smallest value.
# The final value used for the model was ncomp = 28.
plsImp <- varImp(plsFit, scale = FALSE)
plsImp

# GOAL      MISS     SHOT
# shotPlayContinuedOutsideZone 0.093725 0.0143545 0.046927
# shotPlayContinuedInZone      0.089476 0.0146990 0.045558
# shotGoalieFroze              0.072347 0.0458233 0.055375
# shotGeneratedRebound         0.045936 0.0128979 0.026454
# homePenalty1Length           0.017606 0.0031366 0.009034
# shotAngleAdjusted            0.016303 0.0042229 0.009158
# shotTypeTip                  0.008562 0.0145265 0.011369
# shotTypeBack                 0.013377 0.0044338 0.008164
# speedFromLastEvent           0.012939 0.0047303 0.007809
# shotRebound                  0.012070 0.0047358 0.006982
# shotAnglePlusRebound         0.011409 0.0014221 0.005764
# arenaAdjustedYCordAbs        0.010672 0.0019036 0.004853
# defendingTeamForwardsOnIce   0.010005 0.0012337 0.005122
# shotTypeSlap                 0.009856 0.0046125 0.006724
# playerPositionThatDidEventD  0.009460 0.0008847 0.003988
# shotTypeWrist                0.002260 0.0091667 0.005712
# arenaAdjustedXCordABS        0.008747 0.0015637 0.003653
# offWing                      0.008331 0.0016285 0.004394
# homeSkatersOnIce             0.007315 0.0022605 0.004225
# shotAngleReboundRoyalRoad    0.007200 0.0026736 0.004564

plot(plsImp, top=15, scales = list(y = list(cex = .85)))

### Predict the test set
goal.Results_plsFit <- data.frame(obs = Test.NHL_df_event$event)
goal.Results_plsFit$prob <- predict(plsFit, Test.NHL_df_event, type = "prob")[, "GOAL"]
goal.Results_plsFit$pred <- predict(plsFit, Test.NHL_df_event)
goal.Results_plsFit$Label <- ifelse(goal.Results_plsFit$obs == "GOAL",
                                    "True Outcome: GOAL",
                                    "True Outcome: MISS",
                                    "True Outcome: SHOT")

goal.Results_plsFit$obs <- as.factor(goal.Results_plsFit$obs)


defaultSummary(goal.Results_plsFit)
# Accuracy     Kappa 
# 0.7199491 0.3134515

### Create the confusion matrix from the test set.
confusionMatrix(data = goal.Results_plsFit$pred, 
                reference = goal.Results_plsFit$obs,
                positive = "Goal")

# Confusion Matrix and Statistics
# 
# Reference
# Prediction GOAL MISS SHOT
# GOAL 1094  175  233
# MISS    0  267  177
# SHOT    1 3815 9953
# 
# Overall Statistics
# 
# Accuracy : 0.7199         
# 95% CI : (0.7129, 0.727)
# No Information Rate : 0.6594         
# P-Value [Acc > NIR] : < 2.2e-16      
# 
# Kappa : 0.3135         
# 
# Mcnemar's Test P-Value : < 2.2e-16      
# 
# Statistics by Class:
# 
#                      Class: GOAL Class: MISS Class: SHOT
# Sensitivity              0.99909     0.06272      0.9604
# Specificity              0.97209     0.98455      0.2870
# Pos Pred Value           0.72836     0.60135      0.7229
# Neg Pred Value           0.99993     0.73872      0.7893
# Prevalence               0.06968     0.27089      0.6594
# Detection Rate           0.06962     0.01699      0.6333
# Detection Prevalence     0.09558     0.02825      0.8762
# Balanced Accuracy        0.98559     0.52364      0.6237

### ROC curves:
goal.ROC_plsFit <- multiclass.roc(response = goal.Results_plsFit$obs, predictor = goal.Results_plsFit$prob, levels = levels(goal.Results_plsFit$obs))

auc(goal.ROC_plsFit) #Multi-class area under the curve: 0.8715


################################################################################
### 4 glmnet

glmnGrid <- expand.grid(alpha = c(0,  .1,  .2, .4, .6, .8, 1), lambda = seq(.01, .2, length = 40))

set.seed(123)
glmnFit <- train(event ~ .,
                 data = Train.NHL_df_event,
                 method = "glmnet",
                 tuneGrid = glmnGrid,
                 trControl = ctrl)
glmnFit

# glmnet 
# 
# 62867 samples
# 87 predictor
# 3 classes: 'GOAL', 'MISS', 'SHOT' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 56580, 56580, 56580, 56580, 56582, 56580, ... 
# Resampling results across tuning parameters:
#   
#   alpha  lambda      logLoss    AUC        prAUC      Accuracy   Kappa          Mean_F1    Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value  Mean_Precision  Mean_Recall
# 0.0    0.01000000  0.5745667  0.8220204  0.7236207  0.7178487   2.975503e-01  0.6038423  0.6439071         0.7426078         0.6917192            0.8373662            0.6917192       0.6439071  
# 0.0    0.01487179  0.5753150  0.8219803  0.7235866  0.7176896   2.963074e-01  0.6033165  0.6422044         0.7421884         0.6926804            0.8374875            0.6926804       0.6422044  
# 0.0    0.01974359  0.5841154  0.8214515  0.7229595  0.7147310   2.802155e-01  0.5946129  0.6206784         0.7372042         0.6987814            0.8367817            0.6987814       0.6206784  
# 0.0    0.02461538  0.5921863  0.8208919  0.7223152  0.7112953   2.626635e-01  0.5831537  0.5969432         0.7320827         0.7044522            0.8357220            0.7044522       0.5969432  
# 0.0    0.02948718  0.5996047  0.8202979  0.7216606  0.7061256   2.393743e-01  0.5645364  0.5671808         0.7256092         0.7069560            0.8323240            0.7069560       0.5671808  
# 0.0    0.03435897  0.6064556  0.8196800  0.7209386  0.7001606   2.133836e-01  0.5412961  0.5352708         0.7186344         0.7075288            0.8269380            0.7075288       0.5352708  
# 0.0    0.03923077  0.6127913  0.8190323  0.7201360  0.6945138   1.883406e-01  0.5160625  0.5046542         0.7121692         0.7084802            0.8214107            0.7084802       0.5046542  
# 0.0    0.04410256  0.6187085  0.8183578  0.7191861  0.6896305   1.663850e-01  0.4909773  0.4779882         0.7066586         0.7096356            0.8157365            0.7096356       0.4779882  
# 0.0    0.04897436  0.6241504  0.8176745  0.7181394  0.6843812   1.431231e-01  0.4618769  0.4512356         0.7009346         0.7085961            0.8082260            0.7085961       0.4512356  
# 0.0    0.05384615  0.6292316  0.8169582  0.7169408  0.6810249   1.271262e-01  0.4399221  0.4331038         0.6970826         0.7103679            0.8036522            0.7103679       0.4331038  
# 0.0    0.05871795  0.6339938  0.8162181  0.7156284  0.6778913   1.124283e-01  0.4179962  0.4166727         0.6936030         0.7096699            0.7985739            0.7096699       0.4166727  
# 0.0    0.06358974  0.6384266  0.8154592  0.7141950  0.6755054   1.006354e-01  0.3996339  0.4040545         0.6908216         0.7095458            0.7947188            0.7095458       0.4040545  
# 0.0    0.06846154  0.6425469  0.8146743  0.7126414  0.6733739   8.972453e-02  0.3829709  0.3932523         0.6881798         0.7126159            0.7903602            0.7126159       0.3932523  
# 0.0    0.07333333  0.6464399  0.8138502  0.7108950  0.6714015   7.986626e-02  0.3675594  0.3837783         0.6858124         0.7137062            0.7853426            0.7137062       0.3837783  
# 0.0    0.07820513  0.6501550  0.8129881  0.7090323  0.6703039   7.340211e-02  0.3577973  0.3781686         0.6842481         0.7172720            0.7836909            0.7172720       0.3781686  
# 0.0    0.08307692  0.6536022  0.8121114  0.7070641  0.6688883   6.593660e-02  0.3460564  0.3715260         0.6824636         0.7202957            0.7797632            0.7202957       0.3715260  
# 0.0    0.08794872  0.6568479  0.8112122  0.7049821  0.6678861   6.014465e-02  0.3370370  0.3666793         0.6810849         0.7215432            0.7772751            0.7215432       0.3666793  
# 0.0    0.09282051  0.6600119  0.8102480  0.7027706  0.6668362   5.454538e-02  0.3285993  0.3622157         0.6797555         0.7185400            0.7741751            0.7185400       0.3622157  
# 0.0    0.09769231  0.6628654  0.8093286  0.7005646  0.6662159   5.033705e-02  0.3221782  0.3589581         0.6787455         0.7201283            0.7732019            0.7201283       0.3589581  
# 0.0    0.10256410  0.6657123  0.8083281  0.6981377  0.6656433   4.664782e-02  0.3172111  0.3564836         0.6778284         0.7181178            0.7713062            0.7181178       0.3564836  
# 0.0    0.10743590  0.6682857  0.8073767  0.6957723  0.6651025   4.328061e-02  0.3116939  0.3537101         0.6770454         0.7197479            0.7695515            0.7197479       0.3537101  
# 0.0    0.11230769  0.6708741  0.8063456  0.6931585  0.6647685   4.047681e-02  0.3080965  0.3520147         0.6763286         0.7186950            0.7687050            0.7186950       0.3520147  
# 0.0    0.11717949  0.6731931  0.8053963  0.6907665  0.6641640   3.696178e-02  0.3038002  0.3499205         0.6754683         0.7186009            0.7659847            0.7186009       0.3499205  
# 0.0    0.12205128  0.6755403  0.8043583  0.6880606  0.6636232   3.335729e-02  0.2990207  0.3476643         0.6746071         0.7113466            0.7642774            0.7113466       0.3476643  
# 0.0    0.12692308  0.6776712  0.8033831  0.6855628  0.6633528   3.120963e-02  0.2963946  0.3464668         0.6740837         0.7134605            0.7636841            0.7134605       0.3464668  
# 0.0    0.13179487  0.6797724  0.8023717  0.6828569  0.6633051   2.974720e-02  0.2935676  0.3452029         0.6737518         0.7179807            0.7654587            0.7179807       0.3452029  
# 0.0    0.13666667  0.6817984  0.8013556  0.6800954  0.6630187   2.777764e-02  0.2915715  0.3443142         0.6732792         0.7220452            0.7649454            0.7220452       0.3443142  
# 0.0    0.14153846  0.6836719  0.8003933  0.6774781  0.6627006   2.571956e-02  0.2895284  0.3433980         0.6727904         0.7323348            0.7640915            0.7323348       0.3433980  
# 0.0    0.14641026  0.6855633  0.7993731  0.6746304  0.6624143   2.392508e-02  0.2881914  0.3425106         0.6723676         0.7309721            0.7632627            0.7309721       0.3425106  
# 0.0    0.15128205  0.6873104  0.7984238  0.6720284  0.6620166   2.168786e-02  0.2848045  0.3410689         0.6718457         0.7269958            0.7601429            0.7269958       0.3410689  
# 0.0    0.15615385  0.6889897  0.7974754  0.6693533  0.6619212   2.056999e-02  0.2839008  0.3406077         0.6715781         0.7274437            0.7611970            0.7274437       0.3406077  
# 0.0    0.16102564  0.6906833  0.7964801  0.6665546  0.6618099   1.933259e-02  0.2828291  0.3400589         0.6712850         0.7388345            0.7617449            0.7388345       0.3400589  
# 0.0    0.16589744  0.6922364  0.7955610  0.6639121  0.6616190   1.786285e-02  0.2815040  0.3395151         0.6709157         0.7408183            0.7605803            0.7408183       0.3395151  
# 0.0    0.17076923  0.6937349  0.7946591  0.6613149  0.6613327   1.590821e-02  0.2795777  0.3386948         0.6704513         0.7571239            0.7585422            0.7571239       0.3386948  
# 0.0    0.17564103  0.6952447  0.7937116  0.6585981  0.6611100   1.465896e-02  0.2779150  0.3382039         0.6701504         0.7539178            0.7559121            0.7539178       0.3382039  
# 0.0    0.18051282  0.6966770  0.7928076  0.6560275  0.6609191   1.337677e-02  0.2758484  0.3377060         0.6698391         0.7448227            0.7535419            0.7448227       0.3377060  
# 0.0    0.18538462  0.6980094  0.7919689  0.6536293  0.6608077   1.245692e-02  0.2755861  0.3373626         0.6696223         0.7460225            0.7530454            0.7460225       0.3373626  
# 0.0    0.19025641  0.6993509  0.7911012  0.6511982  0.6606646   1.155106e-02  0.2752543  0.3370262         0.6694075         0.7508594            0.7515976            0.7508594       0.3370262  
# 0.0    0.19512821  0.7007014  0.7901934  0.6485785  0.6606169   1.086932e-02  0.2756127  0.3367610         0.6692447         0.7934688            0.7528038            0.7934688       0.3367610  
# 0.0    0.20000000  0.7019039  0.7894063  0.6463312  0.6605374   1.022805e-02  0.2748025  0.3365593         0.6690922         0.7871483            0.7525528            0.7871483       0.3365593  
# 0.1    0.01000000  0.5668170  0.8222075  0.7231672  0.7193599   3.086450e-01  0.6057260  0.6612759         0.7462550         0.6839405            0.8375940            0.6839405       0.6612759 ***  
# 0.1    0.01487179  0.5775582  0.8212151  0.7213934  0.7182941   2.981812e-01  0.6018995  0.6468910         0.7426158         0.6924292            0.8395360            0.6924292       0.6468910  
# 0.1    0.01974359  0.5875685  0.8201285  0.7196951  0.7153196   2.807428e-01  0.5932506  0.6239583         0.7371202         0.6999177            0.8397257            0.6999177       0.6239583  
# 0.1    0.02461538  0.5968078  0.8192130  0.7184038  0.7098319   2.540413e-01  0.5747912  0.5895279         0.7293909         0.7052831            0.8379257            0.7052831       0.5895279  
# 0.1    0.02948718  0.6052907  0.8187203  0.7175436  0.7026102   2.210149e-01  0.5474762  0.5487098         0.7203261         0.7087540            0.8336289            0.7087540       0.5487098  
# 0.1    0.03435897  0.6132133  0.8182315  0.7168286  0.6937662   1.809119e-01  0.5083955  0.5011171         0.7098625         0.7100809            0.8265012            0.7100809       0.5011171  
# 0.1    0.03923077  0.6206570  0.8176964  0.7160887  0.6842540   1.366349e-01  0.4572562  0.4516588         0.6988261         0.7094887            0.8169220            0.7094887       0.4516588  
# 0.1    0.04410256  0.6276659  0.8171069  0.7153791  0.6781141   1.061777e-01  0.4169265  0.4197146         0.6914563         0.7100554            0.8105446            0.7100554       0.4197146  
# 0.1    0.04897436  0.6343160  0.8164770  0.7146059  0.6737080   8.296332e-02  0.3859678  0.3984595         0.6858375         0.7125080            0.8054304            0.7125080       0.3984595  
# 0.1    0.05384615  0.6406105  0.8158415  0.7137166  0.6691110   5.818824e-02  0.3483113  0.3751830         0.6800265         0.7215102            0.7967852            0.7215102       0.3751830  
# 0.1    0.05871795  0.6465736  0.8151944  0.7128091  0.6642753   3.327104e-02  0.3092884  0.3537403         0.6743272         0.7199898            0.7781911            0.7199898       0.3537403  
# 0.1    0.06358974  0.6522402  0.8145563  0.7118666  0.6616826   1.941611e-02  0.2892097  0.3437216         0.6711362         0.6972132            0.7604176            0.6972132       0.3437216  
# 0.1    0.06846154  0.6576518  0.8139008  0.7107359  0.6602828   1.104983e-02  0.2787455  0.3388545         0.6691926         0.6941876            0.7440300            0.6941876       0.3388545  
# 0.1    0.07333333  0.6628172  0.8132117  0.7095754  0.6592807   4.820496e-03  0.2721248  0.3355075         0.6677492         0.6728048            0.7174770            0.6728048       0.3355075  
# 0.1    0.07820513  0.6676204  0.8124832  0.7083744  0.6590262   2.174060e-03  0.2692569  0.3343144         0.6671402         0.6903120            0.6992367            0.6903120       0.3343144  
# 0.1    0.08307692  0.6722713  0.8116713  0.7070940  0.6589625   6.586661e-04  0.2682404  0.3336746         0.6667959         0.6886426            0.6809642            0.6886426       0.3336746  
# 0.1    0.08794872  0.6767427  0.8107699  0.7056088  0.6589307  -1.589473e-04        NaN  0.3333147         0.6666071               NaN            0.6631962                  NaN       0.3333147  
# 0.1    0.09282051  0.6808971  0.8098011  0.7041844  0.6592489   1.213529e-04        NaN  0.3333833         0.6666862               NaN            0.6957472                  NaN       0.3333833  
# 0.1    0.09769231  0.6850218  0.8086344  0.7024612  0.6592807  -7.560068e-05        NaN  0.3333186         0.6666427               NaN            0.6346207                  NaN       0.3333186  
# 0.1    0.10256410  0.6887359  0.8075767  0.7009306  0.6592966  -1.558266e-04        NaN  0.3332921         0.6666251               NaN            0.6272030                  NaN       0.3332921  
# 0.1    0.10743590  0.6924639  0.8062239  0.6991345  0.6594239   3.628776e-05        NaN  0.3333449         0.6666750               NaN            0.7198218                  NaN       0.3333449  
# 0.1    0.11230769  0.6958316  0.8048777  0.6975342  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333  
# Mean_Detection_Rate  Mean_Balanced_Accuracy
# 0.2392829            0.6932575             
# 0.2392299            0.6921964             
# 0.2382437            0.6789413             
# 0.2370984            0.6645130             
# 0.2353752            0.6463950             
# 0.2333869            0.6269526             
# 0.2315046            0.6084117             
# 0.2298768            0.5923234             
# 0.2281271            0.5760851             
# 0.2270083            0.5650932             
# 0.2259638            0.5551379             
# 0.2251685            0.5474381             
# 0.2244580            0.5407160             
# 0.2238005            0.5347954             
# 0.2234346            0.5312084             
# 0.2229628            0.5269948             
# 0.2226287            0.5238821             
# 0.2222787            0.5209856             
# 0.2220720            0.5188518             
# 0.2218811            0.5171560             
# 0.2217008            0.5153778             
# 0.2215895            0.5141716             
# 0.2213880            0.5126944             
# 0.2212077            0.5111357             
# 0.2211176            0.5102752             
# 0.2211017            0.5094774             
# 0.2210062            0.5087967             
# 0.2209002            0.5080942             
# 0.2208048            0.5074391             
# 0.2206722            0.5064573             
# 0.2206404            0.5060929             
# 0.2206033            0.5056720             
# 0.2205397            0.5052154             
# 0.2204442            0.5045731             
# 0.2203700            0.5041771             
# 0.2203064            0.5037725             
# 0.2202692            0.5034925             
# 0.2202215            0.5032168             
# 0.2202056            0.5030028             
# 0.2201791            0.5028257             
# 0.2397866            0.7037654             
# 0.2394314            0.6947534             
# 0.2384399            0.6805392             
# 0.2366106            0.6594594             
# 0.2342034            0.6345179             
# 0.2312554            0.6054898             
# 0.2280847            0.5752424             
# 0.2260380            0.5555854             
# 0.2245693            0.5421485             
# 0.2230370            0.5276048             
# 0.2214251            0.5140338             
# 0.2205609            0.5074289             
# 0.2200943            0.5040236             
# 0.2197602            0.5016284             
# 0.2196754            0.5007273             
# 0.2196542            0.5002352             
# 0.2196436            0.4999609             
# 0.2197496            0.5000347             
# 0.2197602            0.4999807             
# 0.2197655            0.4999586             
# 0.2198080            0.5000099             
# 0.2198080            0.5000000             
# [ reached getOption("max.print") -- omitted 218 rows ]
# 
# logLoss was used to select the optimal model using the smallest value.
# The final values used for the model were alpha = 0.1 and lambda = 0.01.

glmnFitImp <- varImp(glmnFit, scale = FALSE)
glmnFitImp

# glmnet variable importance
# 
# variables are sorted by maximum importance across the classes
# only 20 most important variables shown (out of 87)
# 
# GOAL    MISS      SHOT
# shotPlayContinuedOutsideZone 1.357208 0.57875 6.673e-01
# shotPlayContinuedInZone      1.272716 0.54019 6.214e-01
# shotGoalieFroze              0.699441 0.20792 1.018e+00
# shotGeneratedRebound         0.573667 0.09774 3.650e-01
# arenaAdjustedYCordAbs        0.189969 0.07953 7.233e-05
# shotDistance                 0.179725 0.04720 2.143e-02
# shotTypeWrist                0.000000 0.14654 8.827e-02
# shotTypeSnap                 0.003067 0.13619 2.209e-02
# homePenalty1Length           0.134212 0.00115 2.167e-02
# arenaAdjustedXCordABS        0.102746 0.04369 0.000e+00
# playerPositionThatDidEventD  0.095350 0.00000 9.460e-03
# shotTypeTip                  0.000000 0.09284 1.729e-02
# shotTypeSlap                 0.000000 0.08281 1.039e-02
# shotRebound                  0.048277 0.07612 0.000e+00
# shotTypeBack                 0.000000 0.04388 7.085e-02
# shotAngleAdjusted            0.064070 0.00000 2.996e-02
# locationHomezone             0.000000 0.05532 3.976e-02
# shootingTeamDefencemenOnIce  0.054495 0.00000 3.788e-03
# lastEventxCord_adjusted      0.000000 0.05189 2.645e-02
# isHomeTeam                   0.000000 0.04446 4.693e-02

plot(glmnFitImp, top=15, scales = list(y = list(cex = .85)))

### Predict the test set
goal.Results_glmnFit <- data.frame(obs = Test.NHL_df_event$event)
goal.Results_glmnFit$prob <- predict(glmnFit, Test.NHL_df_event, type = "prob")[, "GOAL"]
goal.Results_glmnFit$pred <- predict(glmnFit, Test.NHL_df_event)
goal.Results_glmnFit$Label <- ifelse(goal.Results_glmnFit$obs == "GOAL",
                                     "True Outcome: GOAL",
                                     "True Outcome: MISS",
                                     "True Outcome: SHOT")

goal.Results_glmnFit$obs <- as.factor(goal.Results_glmnFit$obs)

defaultSummary(goal.Results_glmnFit)
# Accuracy     Kappa 
# 0.7197582 0.3118148 

### Create the confusion matrix from the test set.
confusionMatrix(data = goal.Results_glmnFit$pred, 
                reference = goal.Results_glmnFit$obs)

# Confusion Matrix and Statistics
# 
# Reference
# Prediction GOAL MISS SHOT
# GOAL 1042  140  179
# MISS    3  356  271
# SHOT   50 3761 9913
# 
# Overall Statistics
# 
# Accuracy : 0.7198          
# 95% CI : (0.7127, 0.7268)
# No Information Rate : 0.6594          
# P-Value [Acc > NIR] : < 2.2e-16       
# 
# Kappa : 0.3118          
# 
# Mcnemar's Test P-Value : < 2.2e-16       
# 
# Statistics by Class:
# 
#                      Class: GOAL Class: MISS Class: SHOT
# Sensitivity              0.95160     0.08363      0.9566
# Specificity              0.97818     0.97609      0.2879
# Pos Pred Value           0.76561     0.56508      0.7223
# Neg Pred Value           0.99631     0.74140      0.7740
# Prevalence               0.06968     0.27089      0.6594
# Detection Rate           0.06631     0.02265      0.6308
# Detection Prevalence     0.08661     0.04009      0.8733
# Balanced Accuracy        0.96489     0.52986      0.6223          

### ROC curves:
goal.ROC_glmnFit <- multiclass.roc(response = goal.Results_glmnFit$obs, predictor = goal.Results_glmnFit$prob, levels = levels(goal.Results_glmnFit$obs))

auc(goal.ROC_glmnFit) #Multi-class area under the curve: 0.8263


################################################################################
### 5 Sparse logistic regression

set.seed(123)
spLDAFit <- train(event ~ .,
                  data = Train.NHL_df_event,
                  "sparseLDA",
                  tuneGrid = expand.grid(lambda = c(.1),
                                         NumVars = c(1, 5, 10, 15, 20, 50, 100, 250, 500, 1000)),
                  preProc = c("center", "scale"),
                  trControl = ctrl)

spLDAFit

# Sparse Linear Discriminant Analysis 
# 
# 62867 samples
# 87 predictor
# 3 classes: 'GOAL', 'MISS', 'SHOT' 
# 
# Pre-processing: centered (87), scaled (87) 
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 56580, 56580, 56580, 56580, 56582, 56580, ... 
# Resampling results across tuning parameters:
#   
#   NumVars  logLoss    AUC        prAUC      Accuracy   Kappa      Mean_F1    Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value  Mean_Precision  Mean_Recall  Mean_Detection_Rate
# 1     0.7164134  0.6936724  0.2525638  0.6594239  0.0000000        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080  ***        
# 5     0.8984656  0.8065753  0.6977505  0.7177217  0.2999165  0.5961589  0.6683892         0.7432333         0.6789234            0.8496468            0.6789234       0.6683892    0.2392406          
# 10     0.9234431  0.8110026  0.7114201  0.7199486  0.3178778  0.6034967  0.6770237         0.7496520         0.6753064            0.8375339            0.6753064       0.6770237    0.2399829          
# 15     0.9217073  0.8140520  0.7152164  0.7202825  0.3215852  0.6079708  0.6789457         0.7510655         0.6741297            0.8351954            0.6741297       0.6789457    0.2400942          
# 20     0.9198910  0.8175288  0.7195273  0.7203621  0.3217679  0.6080589  0.6790090         0.7511184         0.6744607            0.8353212            0.6744607       0.6790090    0.2401207          
# 50     0.9164354  0.8204593  0.7248015  0.7208552  0.3256766  0.6123352  0.6809883         0.7525883         0.6749190            0.8336746            0.6749190       0.6809883    0.2402851          
# 100     0.9156349  0.8202662  0.7242821  0.7212846  0.3324944  0.6204016  0.6847001         0.7552985         0.6742145            0.8297224            0.6742145       0.6847001    0.2404282          
# 250     0.9156349  0.8202662  0.7242821  0.7212846  0.3324944  0.6204016  0.6847001         0.7552985         0.6742145            0.8297224            0.6742145       0.6847001    0.2404282          
# 500     0.9156349  0.8202662  0.7242821  0.7212846  0.3324944  0.6204016  0.6847001         0.7552985         0.6742145            0.8297224            0.6742145       0.6847001    0.2404282          
# 1000     0.9156349  0.8202662  0.7242821  0.7212846  0.3324944  0.6204016  0.6847001         0.7552985         0.6742145            0.8297224            0.6742145       0.6847001    0.2404282          
# Mean_Balanced_Accuracy
# 0.5000000             
# 0.7058112             
# 0.7133379             
# 0.7150056             
# 0.7150637             
# 0.7167883             
# 0.7199993             
# 0.7199993             
# 0.7199993             
# 0.7199993             
# 
# Tuning parameter 'lambda' was held constant at a value of 0.1
# logLoss was used to select the optimal model using the smallest value.
# The final values used for the model were NumVars = 1 and lambda = 0.1.


spLDAFitImp <- varImp(spLDAFit, scale = FALSE)
spLDAFitImp

# ROC curve variable importance
# 
# variables are sorted by maximum importance across the classes
# only 20 most important variables shown (out of 87)
# 
# GOAL   MISS   SHOT
# shotPlayContinuedOutsideZone           0.7490 0.7490 0.6834
# shotDistance                           0.7220 0.7220 0.6926
# shotPlayContinuedInZone                0.7092 0.7092 0.6489
# arenaAdjustedXCordABS                  0.6864 0.6864 0.6547
# arenaAdjustedYCordAbs                  0.6820 0.6820 0.6717
# shotGoalieFroze                        0.6241 0.6190 0.6241
# timeSinceLastEvent                     0.6046 0.6046 0.5800
# playerPositionThatDidEventD            0.5929 0.5929 0.5865
# speedFromLastEvent                     0.5720 0.5720 0.5450
# shotRebound                            0.5618 0.5618 0.5499
# playerPositionThatDidEventC            0.5575 0.5575 0.5514
# shotAngleAdjusted                      0.5572 0.5451 0.5572
# shotAnglePlusReboundSpeed              0.5481 0.5481 0.5459
# distanceFromLastEvent                  0.5454 0.5405 0.5454
# shootingTeamAverageTimeOnIce           0.5415 0.5235 0.5415
# shootingTeamAverageTimeOnIceOfForwards 0.5379 0.5205 0.5379
# shotAnglePlusRebound                   0.5378 0.5361 0.5378
# time                                   0.5370 0.5325 0.5370
# shootingTeamMaxTimeOnIce               0.5360 0.5263 0.5360
# shootingTeamMaxTimeOnIceOfForwards     0.5358 0.5219 0.5358


plot(spLDAFitImp, top=15, scales = list(y = list(cex = .85)))

### Predict the test set
goal.Results_spLDAFit <- data.frame(obs = Test.NHL_df_event$event)
goal.Results_spLDAFit$prob <- predict(spLDAFit, Test.NHL_df_event, type = "prob")[, "GOAL"]
goal.Results_spLDAFit$pred <- predict(spLDAFit, Test.NHL_df_event)
goal.Results_spLDAFit$Label <- ifelse(goal.Results_spLDAFit$obs == "GOAL",
                                      "True Outcome: GOAL",
                                      "True Outcome: MISS",
                                      "True Outcome: SHOT")

goal.Results_spLDAFit$obs <- as.factor(goal.Results_spLDAFit$obs)

defaultSummary(goal.Results_spLDAFit)
# Accuracy     Kappa 
# 0.6594337 0.0000000 

### Create the confusion matrix from the test set.
confusionMatrix(data = goal.Results_spLDAFit$pred, 
                reference = goal.Results_spLDAFit$obs)

# Confusion Matrix and Statistics
# 
# Reference
# Prediction  GOAL  MISS  SHOT
# GOAL     0     0     0
# MISS     0     0     0
# SHOT  1095  4257 10363
# 
# Overall Statistics
# 
# Accuracy : 0.6594         
# 95% CI : (0.652, 0.6668)
# No Information Rate : 0.6594         
# P-Value [Acc > NIR] : 0.5037         
# 
# Kappa : 0              
# 
# Mcnemar's Test P-Value : NA             
# 
# Statistics by Class:
# 
#                      Class: GOAL Class: MISS Class: SHOT
# Sensitivity              0.00000      0.0000      1.0000
# Specificity              1.00000      1.0000      0.0000
# Pos Pred Value               NaN         NaN      0.6594
# Neg Pred Value           0.93032      0.7291         NaN
# Prevalence               0.06968      0.2709      0.6594
# Detection Rate           0.00000      0.0000      0.6594
# Detection Prevalence     0.00000      0.0000      1.0000
# Balanced Accuracy        0.50000      0.5000      0.5000 

### ROC curves:
goal.ROC_spLDAFit <- multiclass.roc(response = goal.Results_spLDAFit$obs, predictor = goal.Results_spLDAFit$prob, levels = levels(goal.Results_spLDAFit$obs))

auc(goal.ROC_spLDAFit) #Multi-class area under the curve: 0.651


################################################################################
### 6 Nearest Shrunken Centroids

set.seed(123)
nscFit <- train(event ~ .,
                data = Train.NHL_df_event,
                method = "pam",
                tuneGrid = data.frame(threshold = seq(0, 25, length = 30)),
                trControl = ctrl)
nscFit

# Nearest Shrunken Centroids 
# 
# 62867 samples
# 87 predictor
# 3 classes: 'GOAL', 'MISS', 'SHOT' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 56580, 56580, 56580, 56580, 56582, 56580, ... 
# Resampling results across tuning parameters:
#   
#   threshold  logLoss    AUC        prAUC      Accuracy   Kappa          Mean_F1    Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value  Mean_Precision  Mean_Recall  Mean_Detection_Rate
# 0.000000  0.7587388  0.6880245  0.4533505  0.6557017   2.636628e-02  0.3094978  0.3514260         0.6726533         0.4762071            0.7127834            0.4762071       0.3514260    0.2185672          
# 0.862069  0.7518590  0.7063805  0.4716524  0.6583741   1.018269e-02  0.2845899  0.3413906         0.6688712         0.4946566            0.7165106            0.4946566       0.3413906    0.2194580          
# 1.724138  0.7496832  0.7264575  0.4956807  0.6593284   1.062506e-03  0.2681144  0.3345220         0.6668722         0.4896910            0.7094918            0.4896910       0.3345220    0.2197761          
# 2.586207  0.7515008  0.7414516  0.5193670  0.6593921  -4.770992e-05        NaN  0.3333173         0.6666537               NaN            0.5531255                  NaN       0.3333173    0.2197974          
# 3.448276  0.7550808  0.7470014  0.5325959  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 4.310345  0.7587003  0.7462689  0.5348728  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 5.172414  0.7619975  0.7446817  0.5341279  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 6.034483  0.7650810  0.7436958  0.5343757  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 6.896552  0.7679384  0.7429402  0.5331924  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 7.758621  0.7707088  0.7425928  0.5292713  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 8.620690  0.7733487  0.7429351  0.5273213  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 9.482759  0.7760505  0.7419981  0.5268129  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 10.344828  0.7788039  0.7402370  0.5283663  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 11.206897  0.7814743  0.7414777  0.5341908  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 12.068966  0.7840237  0.7428501  0.5389483  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 12.931034  0.7864760  0.7451791  0.5426697  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 13.793103  0.7889715  0.7477344  0.5449997  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 14.655172  0.7914645  0.7500067  0.5440288  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 15.517241  0.7936971  0.7537217  0.5471587  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 16.379310  0.7958799  0.7590844  0.5558764  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 17.241379  0.7978864  0.7629388  0.5633152  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 18.103448  0.7997061  0.7680622  0.5711294  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 18.965517  0.8011917  0.7703346  0.5766247  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 19.827586  0.8026199  0.7710423  0.5782046  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 20.689655  0.8038484  0.7663366  0.2571355  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 21.551724  0.8050092  0.7662867  0.2571036  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 22.413793  0.8060517  0.6937335  0.2525722  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 23.275862  0.8069047  0.6937335  0.2525722  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 24.137931  0.8077643  0.6937335  0.2525722  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# 25.000000  0.8086302  0.6936724  0.2525638  0.6594239   0.000000e+00        NaN  0.3333333         0.6666667               NaN                  NaN                  NaN       0.3333333    0.2198080          
# Mean_Balanced_Accuracy
# 0.5120396             
# 0.5051309             
# 0.5006971             
# 0.4999855             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 0.5000000             
# 
# logLoss was used to select the optimal model using the smallest value.
# The final value used for the model was threshold = 1.724138.

nscFitImp <- varImp(nscFit, scale = FALSE)
nscFitImp

# pam variable importance
# 
# variables are sorted by maximum importance across the classes
# only 20 most important variables shown (out of 87)
# 
# GOAL      MISS      SHOT
# shotPlayContinuedOutsideZone           -0.36982  0.115693 -0.005470
# shotPlayContinuedInZone                -0.31402  0.107658 -0.008068
# shotDistance                           -0.28307  0.030385  0.010519
# arenaAdjustedYCordAbs                  -0.25159  0.018056  0.012256
# arenaAdjustedXCordABS                   0.22791 -0.028731 -0.005368
# shotGoalieFroze                        -0.20407 -0.203983  0.107710
# shotRebound                             0.19834 -0.043564  0.000000
# playerPositionThatDidEventD            -0.15379  0.012016  0.004400
# shotAnglePlusReboundSpeed               0.15017 -0.017751 -0.001661
# shotGeneratedRebound                   -0.09320 -0.033247  0.025856
# playerPositionThatDidEventC             0.07778 -0.005457  0.000000
# speedFromLastEvent                      0.07031 -0.031066  0.002366
# timeSinceLastEvent                     -0.06277  0.013058  0.000000
# shotAngleAdjusted                      -0.06074  0.000000  0.008014
# shotTypeTip                             0.03707  0.059807 -0.030833
# shootingTeamDefencemenOnIce            -0.05925  0.000000  0.004312
# homePenalty1Length                     -0.05583  0.000000  0.001785
# shootingTeamAverageTimeOnIce            0.04722  0.004739 -0.009285
# shootingTeamMaxTimeOnIce                0.04104  0.000000 -0.005333
# shootingTeamAverageTimeOnIceOfForwards  0.03919  0.004115 -0.008181

plot(nscFitImp, top=15, scales = list(y = list(cex = .85)))

### Predict the test set
goal.Results_nscFit <- data.frame(obs = Test.NHL_df_event$event)
goal.Results_nscFit$prob <- predict(nscFit, Test.NHL_df_event, type = "prob")[, "GOAL"]
goal.Results_nscFit$pred <- predict(nscFit, Test.NHL_df_event)
goal.Results_nscFit$Label <- ifelse(goal.Results_nscFit$obs == "GOAL",
                                    "True Outcome: GOAL",
                                    "True Outcome: MISS",
                                    "True Outcome: SHOT")

goal.Results_nscFit$obs <- as.factor(goal.Results_nscFit$obs)

defaultSummary(goal.Results_nscFit)
# Accuracy       Kappa 
# 0.660006363 0.004307123


### Create the confusion matrix from the test set.
confusionMatrix(data = goal.Results_nscFit$pred, 
                reference = goal.Results_nscFit$obs)

# Confusion Matrix and Statistics
# 
# Reference
# Prediction  GOAL  MISS  SHOT
# GOAL    11     2     5
# MISS     0     6     3
# SHOT  1084  4249 10355
# 
# Overall Statistics
# 
# Accuracy : 0.66            
# 95% CI : (0.6525, 0.6674)
# No Information Rate : 0.6594          
# P-Value [Acc > NIR] : 0.4435          
# 
# Kappa : 0.0043          
# 
# Mcnemar's Test P-Value : <2e-16          
# 
# Statistics by Class:
# 
#                      Class: GOAL Class: MISS Class: SHOT
# Sensitivity             0.010046   0.0014094     0.99923
# Specificity             0.999521   0.9997382     0.00355
# Pos Pred Value          0.611111   0.6666667     0.66006
# Neg Pred Value          0.930942   0.7293391     0.70370
# Prevalence              0.069679   0.2708877     0.65943
# Detection Rate          0.000700   0.0003818     0.65892
# Detection Prevalence    0.001145   0.0005727     0.99828
# Balanced Accuracy       0.504783   0.5005738     0.50139

### ROC curves:
goal.ROC_nscFit <- multiclass.roc(response = goal.Results_nscFit$obs, predictor = goal.Results_nscFit$prob, levels = levels(goal.Results_nscFit$obs))

auc(goal.ROC_nscFit) # Multi-class area under the curve: 0.7347


###############################
###############################

plot.roc(goal.ROC_LR[['rocs']][[1]], type = "s", col = rgb(.2, .2, .2, .2), legacy.axes = TRUE)
plot.roc(goal.ROC_ldaFit[['rocs']][[1]], type = "s", add = TRUE, col = rgb(.2, .2, .2, .2), legacy.axes = TRUE)
plot.roc(goal.ROC_plsFit[['rocs']][[1]], type = "s", add = TRUE, col = rgb(.2, .2, .2, .2), legacy.axes = TRUE)
plot.roc(goal.ROC_glmnFit[['rocs']][[1]],type = "s", add = TRUE, col = rgb(.2, .2, .2, .2), legacy.axes = TRUE)
plot.roc(goal.ROC_spLDAFit[['rocs']][[1]],type = "s", add = TRUE, col = rgb(.2, .2, .2, .2), legacy.axes = TRUE)
plot.roc(goal.ROC_nscFit[['rocs']][[1]],type = "s", add = TRUE, col = rgb(.2, .2, .2, .2), legacy.axes = TRUE)

goalResamples <- resamples(list(glm = logisticReg_event,
                                lda = ldaFit,
                                pls = plsFit,
                                glmnet = glmnFit,
                                sparseLDA = spLDAFit,
                                pam = nscFit))

bwplot(goalResamples, metric = "Accuracy")

